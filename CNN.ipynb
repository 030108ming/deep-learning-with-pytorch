{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO3xI6tcdRQPf9eUHNSSt9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/030108ming/deep-learning-with-pytorch/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "w1lzqOD2E3ZV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert mniist image files into a tensor of 4-Dimensions (# of images ,Height, width, color channel)\n",
        "transform = transforms.ToTensor()"
      ],
      "metadata": {
        "id": "MUhBsfuVGAqt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Data\n",
        "train_data = datasets.MNIST(root='/cnn_data', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "EADsHHhoKHXM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Data\n",
        "test_data = datasets.MNIST(root='/cnn_data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "a4Cqh3FJKgKG"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svDZqaWjK-Ld",
        "outputId": "35c45ed0-6901-4f9d-a48a-ee3195849bd0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: /cnn_data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcMfCACfK_6-",
        "outputId": "edb3ff62-54dc-4153-8027-ff20e1a59bec"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: /cnn_data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small batch size for image ... let's say 10\n",
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
      ],
      "metadata": {
        "id": "bPhyRJQVnwoT"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our CNN  model\n",
        "# Describe convolutional layer and what it's doing( 2 convolutional layers)\n",
        "# this is just an wxample in the next we'll build model\n",
        "conv1 = nn.Conv2d(1, 6, 3, 1)\n",
        "conv2 = nn.Conv2d(6, 16, 3, 1)\n",
        ""
      ],
      "metadata": {
        "id": "sX4DnEGYtdVY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grab 1 MNIST record/iamge\n",
        "for i, (x_train, y_train) in enumerate(train_data):\n",
        "  break"
      ],
      "metadata": {
        "id": "TulLuV_UujnY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bONyvG75u2j5",
        "outputId": "2885495f-86ab-4962-f774-84bc21c83e47"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x_train.view(1, 1, 28, 28)"
      ],
      "metadata": {
        "id": "yyCMhsyDu5dk"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform our first convolution\n",
        "x = F.relu(conv1(x)) # rectified linear unit our our activation function"
      ],
      "metadata": {
        "id": "tHvrnhMFvGgK"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 single image, 6 is the filters we asked for , 26x 26\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms6mYovzvfhX",
        "outputId": "ddfc8d31-1cde-4b46-b18b-d45450b06f4d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 26, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pass thru the pooling layer\n",
        "x = F.max_pool2d(x, 2, 2) # kernal of 2 and stride of 2"
      ],
      "metadata": {
        "id": "VVP73CpFvjRX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape # 26/2 = 13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1eIIN9Twj8B",
        "outputId": "14b2f88d-97df-474d-c940-83c659d6ca93"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 13, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do our second convolutional layer\n",
        "x = F.relu(conv2(x))"
      ],
      "metadata": {
        "id": "eyBRJM10wqCr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape # again , we didn't set padding so we lose 2 pixles around the outside of images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21fb6_BDxFgv",
        "outputId": "99025de1-ebfc-4b48-8f13-45d514caeff3"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 11, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pooling layer\n",
        "x = F.max_pool2d(x, 2, 2)"
      ],
      "metadata": {
        "id": "R58XP5j5xPVx"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape # 11/ 2 =5.5  but we have to round down, becaus you can't invent data to round up"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otfq1VcCxoi7",
        "outputId": "5a4afdba-2a1e-4369-c65c-a940b050208b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "((28-2) / 2- 2) / 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2notu_Jxqra",
        "outputId": "b03d4952-6626-414b-8e0f-8c4c49be605c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.5"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Model class\n",
        "class ConvolutionalNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6,3,1)\n",
        "    self.conv2 = nn.Conv2d(6,16,3,1)\n",
        "    # Fully connected Layer\n",
        "    self.fc1 = nn.Linear(5*5*16, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x,2,2) # 2x2 kernal and stride 2\n",
        "    # second pass\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x,2,2) # 2x2 kernal and stride 2\n",
        "\n",
        "    # Review to flatten it out\n",
        "    x = x.view(-1, 16*5*5) # negative one so that we vary the batch size\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_03YqTdFyHtt"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Createan instance of our Model\n",
        "torch.manual_seed(41)\n",
        "model = ConvolutionalNetwork()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFLoDz-Cydeo",
        "outputId": "b59bd648-20fe-4e2d-a68f-e91186990613"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvolutionalNetwork(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss Function optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # smaller learning rate longer its gonna take to train\n"
      ],
      "metadata": {
        "id": "Lpize_5C1ww6"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import current_thread\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Create Variables to Tracks Things\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "\n",
        "\n",
        "# for loop of Epochs\n",
        "for i in range(epochs):\n",
        "  trn_corr = 0\n",
        "  tst_corr = 0\n",
        "\n",
        "  # Train\n",
        "  for b, (x_train,y_train) in enumerate(train_loader):\n",
        "    b+=1 # start our batches at 1\n",
        "    y_pred = model(x_train) # get predicted values from the train set. not flattened\n",
        "    loss = criterion(y_pred, y_train) # how off are we? compare the predictions to correct answers\n",
        "\n",
        "    predicted = torch.max(y_pred.data,1)[1] # add up the number of correct prediction. Indexed off the first point\n",
        "    batch_corr = (predicted == y_train).sum() # how many we got correct from the batch. True=1, False=0, sum those up\n",
        "    trn_corr  += batch_corr # keep tracks as we along in training.\n",
        "\n",
        "\n",
        "    # Updata our parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    #  print out some results\n",
        "    if b%600 == 0:\n",
        "      print(f'Epoch: {i} Batch : {b} loss: {loss.item()}')\n",
        "\n",
        "\n",
        "  train_losses.append(loss)\n",
        "  train_correct.append(trn_corr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Test\n",
        "  with torch.no_grad(): # no gredient we  don't updata  weights and biases with test\n",
        "    for b,(x_test,y_test) in enumerate(test_loader):\n",
        "      y_val = model(x_test)\n",
        "      predicted = torch.max(y_val.data, 1)[1] # Adding up correct predictions\n",
        "      tst_corr += (predicted == y_test).sum() # T=1 F =0 and sum away\n",
        "\n",
        "  loss = criterion(y_val, y_test)\n",
        "  test_losses.append(loss)\n",
        "  test_correct.append(tst_corr)\n",
        "\n",
        "\n",
        "current_time = time.time()\n",
        "total = current_time - start_time\n",
        "print(f'Training Took : {total/60} minutes!' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vlVMgpx2lTi",
        "outputId": "70b1241c-0e86-48b5-905e-2ac2ca3ae204"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Batch : 600 loss: 0.1623610556125641\n",
            "Epoch: 0 Batch : 1200 loss: 0.1502392590045929\n",
            "Epoch: 0 Batch : 1800 loss: 0.4744560718536377\n",
            "Epoch: 0 Batch : 2400 loss: 0.14238706231117249\n",
            "Epoch: 0 Batch : 3000 loss: 0.007758188061416149\n",
            "Epoch: 0 Batch : 3600 loss: 0.3836284875869751\n",
            "Epoch: 0 Batch : 4200 loss: 0.0038223876617848873\n",
            "Epoch: 0 Batch : 4800 loss: 0.0021286322735249996\n",
            "Epoch: 0 Batch : 5400 loss: 0.0569545142352581\n",
            "Epoch: 0 Batch : 6000 loss: 0.00038789428072050214\n",
            "Epoch: 1 Batch : 600 loss: 0.005851339548826218\n",
            "Epoch: 1 Batch : 1200 loss: 0.3855525553226471\n",
            "Epoch: 1 Batch : 1800 loss: 0.004819948226213455\n",
            "Epoch: 1 Batch : 2400 loss: 0.003216963727027178\n",
            "Epoch: 1 Batch : 3000 loss: 0.0332382395863533\n",
            "Epoch: 1 Batch : 3600 loss: 0.5372857451438904\n",
            "Epoch: 1 Batch : 4200 loss: 0.04561494290828705\n",
            "Epoch: 1 Batch : 4800 loss: 0.0007510822033509612\n",
            "Epoch: 1 Batch : 5400 loss: 0.0001173773780465126\n",
            "Epoch: 1 Batch : 6000 loss: 0.14201366901397705\n",
            "Epoch: 2 Batch : 600 loss: 0.023733172565698624\n",
            "Epoch: 2 Batch : 1200 loss: 0.003455493599176407\n",
            "Epoch: 2 Batch : 1800 loss: 0.0008372392621822655\n",
            "Epoch: 2 Batch : 2400 loss: 0.010705141350626945\n",
            "Epoch: 2 Batch : 3000 loss: 0.008078320883214474\n",
            "Epoch: 2 Batch : 3600 loss: 0.0011862406972795725\n",
            "Epoch: 2 Batch : 4200 loss: 0.038080841302871704\n",
            "Epoch: 2 Batch : 4800 loss: 0.0016068397089838982\n",
            "Epoch: 2 Batch : 5400 loss: 0.138673797249794\n",
            "Epoch: 2 Batch : 6000 loss: 0.2449204921722412\n",
            "Epoch: 3 Batch : 600 loss: 0.007151054684072733\n",
            "Epoch: 3 Batch : 1200 loss: 0.011097034439444542\n",
            "Epoch: 3 Batch : 1800 loss: 0.0017998721450567245\n",
            "Epoch: 3 Batch : 2400 loss: 0.0001049584461725317\n",
            "Epoch: 3 Batch : 3000 loss: 0.0031431831885129213\n",
            "Epoch: 3 Batch : 3600 loss: 0.003668801160529256\n",
            "Epoch: 3 Batch : 4200 loss: 0.0037249946035444736\n",
            "Epoch: 3 Batch : 4800 loss: 0.00015864608576521277\n",
            "Epoch: 3 Batch : 5400 loss: 0.0796482041478157\n",
            "Epoch: 3 Batch : 6000 loss: 0.0808732658624649\n",
            "Epoch: 4 Batch : 600 loss: 0.014099588617682457\n",
            "Epoch: 4 Batch : 1200 loss: 0.0382874570786953\n",
            "Epoch: 4 Batch : 1800 loss: 0.16302265226840973\n",
            "Epoch: 4 Batch : 2400 loss: 0.02186887338757515\n",
            "Epoch: 4 Batch : 3000 loss: 0.0024396399967372417\n",
            "Epoch: 4 Batch : 3600 loss: 0.0013979513896629214\n",
            "Epoch: 4 Batch : 4200 loss: 0.000989563181065023\n",
            "Epoch: 4 Batch : 4800 loss: 0.010317974723875523\n",
            "Epoch: 4 Batch : 5400 loss: 0.16506639122962952\n",
            "Epoch: 4 Batch : 6000 loss: 0.0027098222635686398\n",
            "Training Took : 3.994912632306417 minutes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_E5iMrsS-QVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}